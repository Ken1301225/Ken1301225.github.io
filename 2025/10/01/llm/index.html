<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>llm infra调研与学习 | KEN</title><meta name="author" content="Ken"><meta name="copyright" content="Ken"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#e1d5a5"><meta name="description" content="关于大模型的大模型推理加速的调研和学习">
<meta property="og:type" content="article">
<meta property="og:title" content="llm infra调研与学习">
<meta property="og:url" content="https://ken1301225.github.io/2025/10/01/llm/index.html">
<meta property="og:site_name" content="KEN">
<meta property="og:description" content="关于大模型的大模型推理加速的调研和学习">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ken1301225.github.io/2025/10/01/llm/Pasted%20image%2020251003140905.png">
<meta property="article:published_time" content="2025-09-30T16:00:00.000Z">
<meta property="article:modified_time" content="2025-09-30T16:00:00.000Z">
<meta property="article:author" content="Ken">
<meta property="article:tag" content="llm infra">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ken1301225.github.io/2025/10/01/llm/Pasted%20image%2020251003140905.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "llm infra调研与学习",
  "url": "https://ken1301225.github.io/2025/10/01/llm/",
  "image": "https://ken1301225.github.io/2025/10/01/llm/Pasted%20image%2020251003140905.png",
  "datePublished": "2025-09-30T16:00:00.000Z",
  "dateModified": "2025-09-30T16:00:00.000Z",
  "author": [
    {
      "@type": "Person",
      "name": "Ken",
      "url": "https://Ken1301225.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://ken1301225.github.io/2025/10/01/llm/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#e1d5a5')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#e1d5a5')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'llm infra调研与学习',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/callout.css"><link rel="stylesheet" href="/css/page.css"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-color: #7484a4;"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-landmark"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-signal"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tag"></i><span> 标签</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/2025/10/01/llm/Pasted%20image%2020251003140905.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">KEN</span></a><a class="nav-page-title" href="/"><span class="site-name">llm infra调研与学习</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-landmark"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-signal"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tag"></i><span> 标签</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">llm infra调研与学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-09-30T16:00:00.000Z" title="发表于 2025-10-01 00:00:00">2025-10-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-30T16:00:00.000Z" title="更新于 2025-10-01 00:00:00">2025-10-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/llm/">llm</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="2-llm推理优化"><a class="markdownIt-Anchor" href="#2-llm推理优化"></a> 2. LLM推理优化</h1>
<p>参考综述 <a href="https://arxiv.org/abs/2404.14294">A Survey on Efficient Inference for Large Language Models</a></p>
<p><img src="/2025/10/01/llm/Pasted image 20250928213237.png" alt="img" width="467px"></p>
<ul>
<li>LLMs typically demand higher computational cost, memory access cost and memory usage in their inference process (we will analyse the root causes in the Sec. 2.3), which deteriorates the efficiency indicators (e.g., latency, throughput, power consumption and storage) in the resource-constrained scenarios</li>
<li>效率指标：<strong>存储</strong>，<strong>延迟</strong>，<strong>吞吐量</strong>，能量和电源消耗  &lt;—将大大影响----  计算消耗，访存消耗，存储使用</li>
<li>推理低效的根本原因:
<ul>
<li>模型大小: 极大影响计算消耗, 访存消耗和存储使用</li>
<li>注意力操作: 当输入增长时, 注意力计算成二次方增长 (矩阵计算)</li>
<li>解码过程: 在解码阶段, 模型权重将从chip HBM 加载到 GPU chip, 此外还有KV cache随着输入的增长也会导致内存碎片</li>
</ul>
</li>
</ul>
<h2 id="21-分类"><a class="markdownIt-Anchor" href="#21-分类"></a> 2.1. 分类</h2>
<p><img src="/2025/10/01/llm/Pasted image 20250929143713.png" alt="img" width="502px"></p>
<ul>
<li>加速推理方法的三个层次：数据层次优化，模型层次优化，系统层次优化
<ul>
<li>数据层次优化：通过输入的prompt或者更好的组织输出内容
<ul>
<li>不需要改变原始模型（可能需要添加辅助模型，但是开销相对于训练的损耗不值一提）</li>
</ul>
</li>
<li>模型层面的优化：设计模型框架或者压缩预训练模型
<ul>
<li>前者需要costly预训练或者少量微调成成本来获得模型能力，而后者通常会损失模型性能</li>
</ul>
</li>
<li>系统层面的优化：优化推理引擎和服务系统。
<ul>
<li>前者并不涉及高消耗的模型训练，而后者在模型性能上是无损的</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="22-系统层面的优化"><a class="markdownIt-Anchor" href="#22-系统层面的优化"></a> 2.2 系统层面的优化</h2>
<h3 id="221-推理引擎"><a class="markdownIt-Anchor" href="#221-推理引擎"></a> 2.2.1. 推理引擎</h3>
<p><img src="/2025/10/01/llm/Pasted image 20250929145332 1.png" alt="img" width="592px"></p>
<h4 id="2211-图和算子优化"><a class="markdownIt-Anchor" href="#2211-图和算子优化"></a> 2.2.1.1 🧐<strong>图和算子优化</strong></h4>
<ul>
<li>
<p><strong>运行时分析</strong>： 图15中的分析结果表明，注意力算子和线性算子共同主导运行时间，它们的总时长通常超过推理时长的75%，并且多种算子占用很小的运行时，碎片化了算子执行的时间线并且增加了CPU侧内核启动的损耗，因此现今优化的推理引擎实行了高度融合的算子<br>
<img src="/2025/10/01/llm/Pasted image 20250929145922 1.png" alt="img" width="605px"></p>
</li>
<li>
<p>注意力算子优化：</p>
<ul>
<li><code>FlashAttention</code> ： 将整个注意力操作融合成单个的并且存储高效的算子。将输入矩阵进行分块，消除完整数据加载的需求。</li>
<li><code>FlashDecodeing</code> ： 目标是最大化计算并行化对于解码。在解码过程中，Q矩阵退化为一批向量，这使得在并行度仅限于批大小维度的情况下，填充计算单元变得具有挑战性。FlashDecoding通过沿序列维度引入并行计算来解决这个问题，但是会引起softmax计算的损耗</li>
<li><code>FlashDecoding++</code>：oftmax中的最大值仅作为防止数据溢出的缩放因子。然而，动态最大值会带来显著的同步开销。此外，大量实验表明，在典型的LLM（如Llama2 [239]、ChatGLM [240]）中，超过99.99%的softmax输入都处于特定范围内。因此，FlashDecoding++提出基于预先统计信息来确定缩放因子。这消除了 softmax计算中的同步开销，使得后续操作能够与 softmax计算并行执行</li>
</ul>
</li>
<li>
<p>线性算子优化：</p>
<ul>
<li><code>General Matrix-Vector Multiplication (GEMV)</code>（tensorRT-LLM）</li>
<li><code>FlashDecoding++</code>：当工作负载变化时，存在两个挑战：并行度低和内存访问瓶颈
<ul>
<li>采用精密的分块测率提高并行性以及利用双倍缓冲区技术来隐藏访存延迟</li>
<li>线性操作在传统llm中通常有固定的大小，建立启发式筛选机制：基于不同的输入大小动态的选择不同的线性算子
<ul>
<li>动态算子选择包括：FastGEMV [244]、FlatGEMM以及 cuBLAS [242]、[243]库提供的 GEMM</li>
</ul>
</li>
</ul>
</li>
<li><code>MegaBlocks</code>：解决MoE FFN层优化。把MoE FFN计算形式化成块稀疏算子并且提出定制GPU核来加速</li>
</ul>
</li>
<li>
<p>图优化：</p>
<ul>
<li>内核融合
<ul>
<li>减少内存访问</li>
<li>减轻内核启动损失</li>
<li>提高并行化</li>
</ul>
</li>
<li><code>FlashAttention</code>：融合注意力计算，减少访问注意力结果的损失</li>
<li><code>BytyTransformer</code> and <code>deepspeed</code>：融合轻量算子（如残差连接层，层归一化，激活函数）到先前的线性算子。</li>
<li>内核融合技术也被用于提高大语言模型（LLM）推理的利用率。查询（Query）、键（Key）和值（Value）矩阵的投影原本是三个独立的线性运算，现被融合为一个线性算子以部署在现代 GPU上。目前，内核融合技术已在 LLM推理实践中得到应用，高度优化的推理引擎在运行时仅使用少数融合内核。例如，在<code>FlashDecoding++</code> [231] 的实现中，一个 transformer块仅集成了七个融合内核。借助上述算子和内核融合优化，FlashDecoding++相较于 HuggingFace实现实现了高达4.86倍的加速。</li>
</ul>
</li>
</ul>
<h4 id="2212-推测解码投机解码"><a class="markdownIt-Anchor" href="#2212-推测解码投机解码"></a> 2.2.1.2. 🧐推测解码(投机解码)</h4>
<p>这种方法的核心思想包括使用一个较小的模型(称为草稿模型)来有效地预测几个后续令牌，然后使用目标大型语言模型并行验证这些预测。<br>
该方法旨在使LLM能够在通常单次推理所需的时间内生成多个标记。<br>
<img src="/2025/10/01/llm/Pasted image 20250929161434 1.png" alt="img" width="481px"></p>
<ul>
<li>两个步骤
<ul>
<li><strong>Draft Construction</strong>：部署草稿模型生成子序列tokens，并行地或者自回归的方式
<ul>
<li>传统解码技术采用两种基础的采样策略：
<ul>
<li><code>贪心采样</code>:在每个解码步骤中选择概率最高的标记以生成特定的输出序列
<ul>
<li><code>BLockwise Parallel Decoding</code>：保证draft tokens 准确地匹配贪心采样生成的序列来保证输出token的等价性</li>
</ul>
</li>
<li><code>核采样</code>:从概率分布中采样标记，每次运行都会生成不同的标记序列
<ul>
<li><code>Speculative Sampling</code>（投机采样）：保留输出分布的等价性<br>
<img src="/2025/10/01/llm/Pasted image 20250929163246.png" alt="img" width="444px"></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>Draft Verification</strong>： 部署目标模型计算单个LLM推理步中所有草稿tokens的条件概率，随后依次决定是否接受每个token是否接受。接受度代表每个推理步平均被接受的draft tokens的个数，被视作衡量推测解码算法的性能的重要指标
<ul>
<li><code>Token tree verifier</code>：用树结构来表示draft token集，使用树注意力机制来执行验证过程
<ul>
<li>被证明是有效的提高推测解码的性能的方法</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>主要研究方向
<ul>
<li><strong>设计更好的draft model</strong>
<ul>
<li><code>DistillSpec</code> [220]直接从目标LLM中蒸馏出一个更小的草稿模型。</li>
<li><code>SSD</code> [221]涉及从目标LLM中自动识别一个子模型（模型层的子集）作为草稿模型，从而无需单独训练草稿模型。</li>
<li><code>OSD</code> [222] 在在线LLM服务中动态调整草稿模型的输出分布，以匹配用户查询分布。它通过监控LLM拒绝的草稿令牌，并利用这些数据通过蒸馏来优化草稿模型，从而实现这一目标。</li>
<li><code>PaSS</code> [223]提出将目标LLM本身用作草稿模型，将可训练令牌（前瞻令牌）融入输入序列，以实现后续令牌的同时生成。</li>
<li><code>REST</code> [224]引入了一种基于检索的推测解码方法，采用非参数检索数据存储作为草稿模型</li>
<li><code>SpecInfer</code> [225]提出了一种集体增强调优技术，使一组草稿模型的输出分布与目标LLM的输出分布对齐。</li>
<li><code>Lookahead decoding</code>[228]涉及并行生成目标LLM的n元语法，以辅助生成草稿令牌。</li>
<li><code>Medusa</code> [48]专门微调LLM的多个头，用于生成后续草稿令牌。</li>
<li><code>Eagle</code> [229]采用一种名为自回归头的轻量级Transformer层，以自回归方式生成草稿令牌，并将来自目标LLM的丰富上下文特征整合到草稿模型的输入中。</li>
</ul>
</li>
<li><strong>设计更高效的draft construction 方式</strong>
<ul>
<li>传统方法通常会生成单一的草稿 token序列，这给验证带来了挑战。对此，Spectr[230]主张生成多个草稿 token序列，并采用 k顺序草稿选择技术对 k个序列同时进行验证。该方法利用推测采样，确保输出分布的等价性。</li>
<li>类似地，SpecInfer[225]也采用了类似的方法。然而，与 Spectr不同的是，SpecInfer 将草稿 token序列合并成一棵“token树”，并引入树注意力机制进行验证。这种策略被称为“token树验证器”。由于其有效性，token树验证器已被众多推测解码算法广泛采用[48], [224], [226], [229]。</li>
<li>除了这些研究外，阶段推测解码[226]和级联推测草稿生成（CS Drafting）[227]提出通过将推测解码直接集成到 token生成过程中来加速草稿构建。</li>
</ul>
</li>
</ul>
</li>
<li>效率分析<br>
<img src="/2025/10/01/llm/Pasted image 20251007214515.png" alt="img" width="800px">
<ul>
<li><code>Eagle</code>：
<ul>
<li>采用自回归方法，直接利用先前生成tokens</li>
<li>从原始llm的token和draft model中的token集成了丰富的特征，从而提高了推测解码的准确性</li>
</ul>
</li>
</ul>
</li>
<li>端到端加速通过这些方法通常会低于接受率。因为draft model的生成成本通常不能忽略</li>
</ul>
<h3 id="222-服务系统"><a class="markdownIt-Anchor" href="#222-服务系统"></a> 2.2.2. 服务系统</h3>
<p><img src="/2025/10/01/llm/Pasted image 20250929165422.png" alt="img" width="650px"></p>
<h4 id="2221-存储管理"><a class="markdownIt-Anchor" href="#2221-存储管理"></a> 2.2.2.1 存储管理</h4>
<ul>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>S</mi><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">S^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span> : 预先为每个请求估计一个最大生成长度上界, 防止浪费预分配的空间
<ul>
<li>但是如果不能得到连续的空间分配, 那么这个方法将会失效 (页面碎片)</li>
</ul>
</li>
<li><code>vllm</code> : 通过分页的方式存储 KV cache
<ul>
<li>先分配一个尽可能大的存储空间, 分成相等大小的多个物理块, 当请求到来, vllm动态的非连续地映射生成的KV cache 到预先分配的物理块中</li>
</ul>
</li>
<li><code>LightLLM</code> : 使用更细粒度的方式存储KV cache
<ul>
<li>LightLLM不再以块为单位，而是将单个token的KV缓存作为一个单元，从而使生成的KV缓存始终能填满预分配的空间。</li>
</ul>
</li>
</ul>

<div class="callout" data-callout="attention">
<div class="callout-title">
<div class="callout-title-icon">
<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-alert-triangle"><path d="m21.73 18-8-14a2 2 0 0 0-3.48 0l-8 14A2 2 0 0 0 4 21h16a2 2 0 0 0 1.73-3Z"/><path d="M12 9v4"/><path d="M12 17h.01"/></svg>
</div>
<div class="callout-title-inner">分页与访存的平衡</div>
</div>
<div class="callout-content"><p>通过分页的方式虽然能减轻存储空间的浪费, 但是注意力算子在使用KV cache的时候需要繁琐的寻址 (而非连续的方式寻址)</p>
<blockquote>
<ol>
<li>PageAttention by vLLM : 存储的头部维度有16字节为K cache的连续向量</li>
<li>FlashInfer : 为KV缓存协调多样化的数据布局，并辅以适当设计的内存访问方案。注意力算子的优化与分页KV缓存存储相结合，仍是服务系统发展中的前沿挑战。</li>
</ol>
</blockquote>
</div></div><h4 id="2222-连续批处理"><a class="markdownIt-Anchor" href="#2222-连续批处理"></a> 2.2.2.2 连续批处理</h4>
<p>The request lengths in a batch can be different, leading to low utilization when shorter requests are finished and longer requests are still running</p>
<ul>
<li><code>ORCA</code> : 计算过程包含多个迭代过程, 每个迭代要么处在与填充阶段要么处于解码阶段.
<ul>
<li>不同的请求可以在迭代层次被批处理</li>
<li>这个工作在迭代层次下批处理线性算子, 在序列的维度拼接不同的请求</li>
</ul>
</li>
<li><code>vLLM</code> ： 使得不同KV cache长度被批处理</li>
<li><code>Sarathi</code>， <code>DeepSpeed-FastGen</code> and <code>AsrathiServe</code> 引入了划分-融合（split-and-fuse）方法，来把预填充请求和解码请求批处理
<ul>
<li>在序列维度划分长预填充请求，并且和多个短解码请求批处理</li>
<li>平衡不同迭代次数的工作负载，并且显著减少了尾延迟</li>
<li>Split-and-fuse 技术前提是在与填充阶段，请求可以被划分成离散的块。块填充方法论涉及在序列维度划分预填充请求</li>
</ul>
</li>
</ul>
<h4 id="2223-调度策略"><a class="markdownIt-Anchor" href="#2223-调度策略"></a> 2.2.2.3 🧐<strong>调度策略</strong></h4>
<ul>
<li><code>Head-of-line blocking</code>（队头阻塞） 在长请求被赋予优先时发生
<ul>
<li>具体而言，内存使用量会随着长请求的增加而迅速增长，当系统耗尽内存容量时，会阻碍后续请求的处理</li>
</ul>
</li>
<li><code>vLLM</code>和<code>LightLLM</code>使用的是简单的first come first serve的原则</li>
<li><code>DeepSpeed-FastGen</code> 优先处理解码请求</li>
<li><code>FastServe</code> 提出一个优先调度策略优化对头阻塞问题，实现low job completion time
<ul>
<li>采用多级反馈队列</li>
<li>对于解码阶段，由于位置的请求长度，先预测长度以及使用skip-join 的方式找到合适的优先级</li>
</ul>
</li>
<li><code>VTC</code> 讨论公平性。
<ul>
<li>引入一个基于token数量的成本函数，来衡量公平性，并且采用公平调度的方式来保证公平性</li>
</ul>
</li>
</ul>
<h4 id="2224-分布式系统"><a class="markdownIt-Anchor" href="#2224-分布式系统"></a> 2.2.2.4. 分布式系统</h4>
<p>由于预填充阶段是计算密集的，解码阶段是存储密集的。</p>
<ul>
<li><code>Split-wise</code>和 <code>TetriInfer</code> 和 <code>DistServe</code> 证明了将请求的预填充和解码步骤分离的高效性</li>
<li><code>SpotServe</code> ：抢占式GPU实例。解决了动态并行控制和示例迁移</li>
<li><code>Infinite-LLM</code> 将分页KV cache 方法拓展到分布式云环境</li>
</ul>
<h3 id="223-llm-框架对比"><a class="markdownIt-Anchor" href="#223-llm-框架对比"></a> 2.2.3. LLM 框架对比</h3>
<p><img src="/2025/10/01/llm/Pasted image 20251002155532.png" alt="img" width="694px"></p>
<ul>
<li>结果显示：<code>FlashDecoding++</code> 和 <code>TensorRT-LLM</code> 推理速度表现得比其他的方法更出色</li>
</ul>
<h3 id="224-未来的方向以及一些建议"><a class="markdownIt-Anchor" href="#224-未来的方向以及一些建议"></a> 2.2.4. 未来的方向以及一些建议</h3>
<p>系统级优化在提高效率的同时不会降低准确性，因此在LLM推理实践中变得普遍。推理优化也适用于服务。最近，算子优化与实际服务场景紧密结合，例如，RadixAttention[50]是专为前缀缓存设计的，tree attention[225]用于加速推测解码验证。应用和场景的迭代将继续对算子开发提出新的要求。鉴于实际服务系统中固有的多方面目标，如作业完成时间、系统吞吐量和公平性，调度策略的设计相应地变得复杂。在LLM服务领域，请求长度不确定，现有文献通常依靠预测机制来辅助调度策略的设计。然而，当前预测器[248]的效果未达到理想标准，表明服务调度策略的开发仍有改进和优化的潜力</p>
<h2 id="23-模型层次的优化"><a class="markdownIt-Anchor" href="#23-模型层次的优化"></a> 2.3. 模型层次的优化</h2>
<h3 id="231-高效结构设计"><a class="markdownIt-Anchor" href="#231-高效结构设计"></a> 2.3.1. 🧐高效结构设计</h3>
<p><img src="/2025/10/01/llm/Pasted image 20251002173118.png" alt="img" width="691px"></p>
<h4 id="2311-高效ffn设计"><a class="markdownIt-Anchor" href="#2311-高效ffn设计"></a> 2.3.1.1. 高效FFN设计</h4>
<ul>
<li><strong>专注于优化专家权重的获取过程以及使得这些专家更加轻量化</strong>
<ul>
<li><code>MoEfication</code> ：将一个非MoE LLM通过预训练权重转换为一个MoE版本
<ul>
<li>消除昂贵的对于MoE模型的预训练</li>
<li>将预训练大模型的FFN神经元划分为多个组，每个组的神经元同步的被激活函数激活</li>
<li>然后，重构每组神经元为一个专家</li>
</ul>
</li>
<li><code>Sparse Upcycling</code> ： 从密集模型的检查点直接初始化MoE-base LLM权重
<ul>
<li>MoE-based LLM 中的专家是密集模型中的FFN的复制体</li>
</ul>
</li>
<li><code>MPOE</code>： 通过Matrix Product Operators decomposition （矩阵乘积算子分解）减少MoE-based LLMs的参数量
<ul>
<li>分解每个FFN的权重矩阵为一个全局共享张量（包含全局信息以及一个捕获特殊特征的局部辅助张量的集合）</li>
</ul>
</li>
</ul>
</li>
<li><strong>设计路由模型</strong>
<ul>
<li><code>Switch Transformer</code>：引入一个额外的损失，称为负载平衡损失，用于惩罚路由模型的不均衡分配
<ul>
<li>这个损失被形式化成token分配向量和一个均匀分布向量的点乘</li>
</ul>
</li>
<li><code>BASE</code>： 端到端地为每一个专家学习一个嵌入，将专家分配给和它的嵌入相似度高的token
<ul>
<li>形式化了一个线性分配问题，并且使用拍卖算法高效解决这个问题</li>
</ul>
</li>
<li><code>Expert Choice</code>：
<ul>
<li>每个专家通过嵌入相似性独立的选择top-K的tokens</li>
</ul>
</li>
</ul>
</li>
<li><strong>提升训练方法</strong>
<ul>
<li><code>SE-MoE</code>： 引入辅助损失，路由z-loss
<ul>
<li>发现：由softmax 算子引入的指数函数会加剧舍入误差，并导致训练不稳定。</li>
<li>Z-loss惩罚输入到指数函数中的大logits，最小化每轮的舍入误差</li>
</ul>
</li>
<li><code>StableMoE</code>： 指出路由波动问题，导致专家分配在训练和推理阶段的不一致性
<ul>
<li>对于相同的输入token，在训练时被分配给不同的专家，但是在推理时只分配一个专家</li>
<li>首先，学习一个路由策略，并且使得它在骨干网络和推理阶段固定</li>
</ul>
</li>
<li><code>SMoE-Dropout</code>：在训练过程中，逐渐在增加激活的专家个数</li>
<li><code>GLaM</code>： 预训练并发布了一些系列带有不同参数量的模型，并且对比了他们和密集型LLM在few-shot 任务上的性能</li>
<li><code>Mixtral 8 $\times$ 7B</code>: 是一个开源的模型. 在推理阶段, 它使用13b激活参数, 并且对比LLaMA-2-70B模型实现了更好的性能
<ul>
<li>每一层由8个FFN专家, 并且在推理时每个token被分配给2个专家</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="2312-高效注意力设计"><a class="markdownIt-Anchor" href="#2312-高效注意力设计"></a> 2.3.1.2. 高效注意力设计.</h4>
<ul>
<li><code>Multi-Query Attention</code>:
<ul>
<li>（MQA）共享KV cache 在多个注意力头之间，同时保留不同的Q矩阵</li>
<li><code>Grouped-query attention</code>（GQA）： 把多个注意力头分组，对于每个组存储一个简单的KV cache。</li>
</ul>
</li>
<li><code>Low-Complexity Attention</code>：
<ul>
<li><code>Kernel-based Attention</code>： kernel <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϕ</mi></mrow><annotation encoding="application/x-tex">\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">ϕ</span></span></span></span> , 用一个核transformer特征映射 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϕ</mi><mo stretchy="false">(</mo><mi>Q</mi><mo stretchy="false">)</mo><mi>ϕ</mi><mo stretchy="false">(</mo><mi>K</mi><msup><mo stretchy="false">)</mo><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\phi(Q)\phi(K)^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0913309999999998em;vertical-align:-0.25em;"></span><span class="mord mathdefault">ϕ</span><span class="mopen">(</span><span class="mord mathdefault">Q</span><span class="mclose">)</span><span class="mord mathdefault">ϕ</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span> 线性点乘, 近似非线性的softmax算子<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal"><mi mathvariant="normal">S</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi><mo stretchy="false">(</mo><mi mathvariant="normal">Q</mi><msup><mi mathvariant="normal">K</mi><mi mathvariant="normal">T</mi></msup><mo stretchy="false">)</mo></mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\operatorname{Softmax(QK^T)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0913309999999998em;vertical-align:-0.25em;"></span><span class="mop"><span class="mord mathrm">S</span><span class="mord mathrm">o</span><span class="mord mathrm" style="margin-right:0.07778em;">f</span><span class="mord mathrm">t</span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span><span class="mord mathrm">(</span><span class="mord mathrm">Q</span><span class="mord"><span class="mord mathrm">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathrm mtight">T</span></span></span></span></span></span></span></span><span class="mord mathrm">)</span></span></span></span></span>
<ul>
<li>它通过先计算 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϕ</mi><mo stretchy="false">(</mo><mi>K</mi><msup><mo stretchy="false">)</mo><mi>T</mi></msup><mi>V</mi></mrow><annotation encoding="application/x-tex">\phi(K)^TV</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0913309999999998em;vertical-align:-0.25em;"></span><span class="mord mathdefault">ϕ</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span></span></span></span> 再与 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϕ</mi><mo stretchy="false">(</mo><mi>Q</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\phi(Q)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">ϕ</span><span class="mopen">(</span><span class="mord mathdefault">Q</span><span class="mclose">)</span></span></span></span> 相乘从而避免了 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">QK^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.035771em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">Q</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span> 的传统二次计算量</li>
<li>Linear Transformer: 它采用了 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϕ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="normal">elu</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\phi(x)= \operatorname{elu}(x)+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">ϕ</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop"><span class="mord mathrm">e</span><span class="mord mathrm">l</span><span class="mord mathrm">u</span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> 作为核函数</li>
<li>Performers 以及 RFA 提出了使用随机特征投影来更好地近似softmax函数</li>
<li>PolySketchFormer 使用了多项式函数以及草图技术来近似softmax函数</li>
</ul>
</li>
<li><code>Low-Rank Attention</code> : 使用了,将KV矩阵的token维度压缩为一个固定的, 更小的长度
<ul>
<li>主流的研究 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>n</mi><mo>×</mo><mi>d</mi></mrow></msup><mo>→</mo><msup><mi>X</mi><mo mathvariant="normal">′</mo></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>k</mi><mo>×</mo><mi>d</mi></mrow></msup><mo separator="true">,</mo><mi mathvariant="normal">其</mi><mi mathvariant="normal">中</mi><mi>X</mi><mi mathvariant="normal">是</mi><mi>K</mi><mi mathvariant="normal">或</mi><mi mathvariant="normal">者</mi><mi>V</mi></mrow><annotation encoding="application/x-tex">X\in\mathbb{R}^{n\times d }\to X&#x27;\in\mathbb{R}^{k\times d},其中X是K或者V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72243em;vertical-align:-0.0391em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.790992em;vertical-align:-0.0391em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.043548em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord cjk_fallback">其</span><span class="mord cjk_fallback">中</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mord cjk_fallback">是</span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mord cjk_fallback">或</span><span class="mord cjk_fallback">者</span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span></span></span></span></li>
<li>其他研究: 采用线性投影压缩token维度, 对于KV矩阵分别乘以投影矩阵 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">P_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">P_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></li>
<li><code>Linformer</code>: 首先提出并分析attention 映射的低秩特性.</li>
<li><code>LRT</code>: 提出同时使用低秩transformer于注意力块, FFN</li>
<li><code>FLuRKA</code>: 结合低秩transformer以及核化的注意力矩阵
<ul>
<li>首先减少KV矩阵的token维度, 然后采用核方法于Q和低秩的K</li>
</ul>
</li>
</ul>
</li>
<li><code>Luna</code> 以及<code>Set Transformer</code> 使用更小的queries 来压缩K和V矩阵
<ul>
<li>Luna涉及额外的query矩阵有固定的长度k.
<ul>
<li>更小的query于原始的上下文矩阵做注意力操作称为 pack attention, 将上下文矩阵压缩到 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi mathvariant="double-struck">R</mi><mrow><mi>k</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbb{R}^{k\times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></li>
<li>相似的, 采用原始的Q矩阵和压缩的KV矩阵做注意力, 称为unpack attentioin</li>
<li>额外的quey矩阵可以是科学系的参数或者从前面的层中获得</li>
</ul>
</li>
<li>Set Transformers: 引入长度固定的 inducing points 向量</li>
<li>Funnel Tansformer 使用pooling算子来逐渐压缩Q矩阵的序列长度</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="2313-替代transformer"><a class="markdownIt-Anchor" href="#2313-替代transformer"></a> 2.3.1.3. 🧐替代Transformer</h4>
<p>在该研究领域内，两个突出的研究方向受到了广泛关注。</p>
<ul>
<li>其中一个研究方向专注于<strong>状态空间模型</strong>（State Space Model, SSM），该模型基于HiPPO理论[62]将序列建模表述为一种<strong>递归变换</strong>。
<ul>
<li>与基于注意力的Transformer相比，SSM在输入序列长度方面表现出线性计算和内存复杂度，这增强了其在处理长上下文序列时的效率。在本综述中，SSM指的是满足以下两个特性的一系列模型架构：
<ol>
<li>它们基于HiPPO[62]和LSSL[63]提出的以下公式对序列进行建模:<br>
<img src="/2025/10/01/llm/Pasted image 20251007220320.png" alt="img" width="200px">其中，A、B和C表示转移矩阵，x表示中间状态，u表示输入序列。</li>
<li>他们基于HiPPO理论[62]设计了转移矩阵A。具体来说，HiPPO提出通过将输入序列投影到一组多项式基上，将其压缩为一系列系数（即状态）。</li>
</ol>
</li>
<li>多项研究致力于<strong>改进转移矩阵A的参数化或初始化</strong>。这涉及到在状态空间模型（SSM）中优化矩阵的构建或初始化方式，以增强其在序列建模任务中的有效性和性能。
<ul>
<li><code>LSSL</code> [63]首先提出使用HiPPO设计的最优转移矩阵HiPPO-LegS对A进行初始化。<br>
<img src="/2025/10/01/llm/Pasted image 20251007221325.png" alt="img" width="500px"></li>
<li>然而，计算该卷积核的成本很高，因为它需要多次与A相乘。为此，<code>S4</code> [64]、<code>DSS</code> [65] 和 <code>S4D</code> [66]提出对矩阵A进行对角化处理，从而加快计算速度。这可视为对转移矩阵A的一种参数化技术。</li>
<li>早期的SSM对每个输入维度独立处理，导致可训练参数数量庞大。为提高效率，<code>S5</code> [70]提出使用单组参数同时处理所有输入维度。基于此结构，S5引入了一种基于标准HiPPO矩阵的A参数化和初始化方法。</li>
<li><code>Liquid S4</code> [69] 和<code>Mamba</code> [73]则以输入依赖的方式对转移矩阵进行参数化，进一步增强了SSM的建模能力。此外，<code>S5</code> [70] 和<code>Mamba</code> [73]均采用并行扫描技术，无需卷积操作即可实现高效的模型训练。该技术在现代GPU硬件上的实现和部署具有优势。</li>
</ul>
</li>
<li>另一类研究旨在基于状态空间模型（SSMs）<strong>设计更好的模型架构</strong>。
<ul>
<li><code>GSS</code> [67] 和 <code>BiGS</code> [72] 将门控注意力单元（GAU）[102]与状态空间模型相结合。具体来说，它们将门控注意力单元中的注意力操作替换为状态空间模型操作。</li>
<li><code>BST</code> [71] 将状态空间模型与所提出的块Transformer相结合，后者引入了强大的局部归纳偏置。</li>
<li><code>H3</code> [68]观察到状态空间模型在回忆早期标记和比较序列中的标记方面存在不足。为此，它建议在标准状态空间模型操作之前添加一个移位状态空间模型操作，用于将输入标记直接移位到状态中。</li>
<li><code>MambaFormer</code> [74] 通过将Transformer中的前馈网络（FFN）层替换为状态空间模型层，将标准Transformer与状态空间模型相结合。</li>
<li><code>Jamba</code> [103]引入了另一种结合Transformer和状态空间模型的方法，即在状态空间模型中添加四个Transformer层。</li>
<li><code>DenseMamba</code>[104]探讨了传统状态空间模型中隐藏状态退化的问题，并在状态空间模型架构中引入密集连接，以在模型更深层保留细粒度信息。</li>
<li><code>BlackMamba</code> [105] 和 <code>MoE-Mamba</code> [106]提出使用混合专家（MoE）技术增强状态空间模型，以在保持模型性能的同时优化训练和推理效率。</li>
</ul>
</li>
</ul>
</li>
<li>此外，其他研究主要侧重于采用<strong>长卷积或设计类注意力公式来对序列进行建模</strong>状态空间模型。<br>
<img src="/2025/10/01/llm/Pasted image 20251007220518.png" alt="img" width="800px"></li>
</ul>
<h3 id="232-模型压缩"><a class="markdownIt-Anchor" href="#232-模型压缩"></a> 2.3.2. 模型压缩</h3>
<p><img src="/2025/10/01/llm/Pasted image 20251002201410.png" alt="img" width="686px"></p>
<h4 id="2321-量化"><a class="markdownIt-Anchor" href="#2321-量化"></a> 2.3.2.1 量化</h4>
<p>将模型权重和激活从<strong>高位宽变为低位宽</strong><br>
<img src="/2025/10/01/llm/Pasted image 20251002201254 1.png" alt="img" width="364px"></p>
<ul>
<li>效率分析：
<ul>
<li>
<p>再预填充阶段的延迟被高精度的CUDA核心的计算性能限制，未来解决这个挑战，现有的方法使用低精度的张量核心来计算量化的权重和激活值</p>
<ul>
<li>Weight-Activation Quantization ： Activations 在线量化再每个GEMM操作前进行<br>
<img src="/2025/10/01/llm/Pasted image 20251002201844 1.png" alt="img" width="470px"></li>
</ul>
</li>
<li>
<p>相反，在解码阶段，延迟主要是受到权重张量加载影响。现有技术通过只量化权重来加速内存读取</p>
<ul>
<li>Weight-only Quantization： 涉及离线量化权重，然后将低精度权重反量化为FP16的格式进行计算<br>
<img src="/2025/10/01/llm/Pasted image 20251002201818.png" alt="img" width="474px"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="23211-后训练量化"><a class="markdownIt-Anchor" href="#23211-后训练量化"></a> 2.3.2.1.1. 后训练量化</h5>
<p>对于训练模型进行量化，无需重新训练，可能是一个开销大的过程</p>
<ul>
<li>Weight-only 量化：
<ul>
<li><code>GPTQ</code>：
<ul>
<li>在传统的OBQ算法上进行构建
<ul>
<li>OBQ利用权重矩阵每行的最优量化顺序，该顺序由相对于未量化权重的 Hessian矩阵的重建误差指导。</li>
<li>在每个量化步后，OBQ迭代的调整未量化的权重来减少重构误差</li>
<li>然而频繁的更新海森矩阵增加了计算复杂度</li>
</ul>
</li>
<li>GPTQ通过采用统一的从左到右顺序对每一行进行量化，从而简化了这一过程，避免了大量Hessian矩阵更新的需求。该策略通过仅在量化一行时计算海森矩阵，然后将计算结果用于后续行，从而显著降低了计算需求，加快了整体量化过程。</li>
</ul>
</li>
<li><code>LUT-GEMM</code>：提出一个新的反量化方法
<ul>
<li>使用查找表来减少反量化的损失</li>
<li>非均匀的量化方法，称为Binary-Codeing Quantization（BCQ</li>
</ul>
</li>
<li><code>AWQ</code>： 权重通道对于性能的影响不同，特别强调那些与激活中表现出异常值的输入通道对齐的权重通道
<ul>
<li>采用重参数化方法：通过网格搜索选择重参数化系数，最小化重构误差</li>
</ul>
</li>
<li><code>OWQ</code>：观察到量化的困难度与激活值的离群点有关。
<ul>
<li>采用混合精度量化策略：找到权重矩阵中的弱列，分配高精度给这些特殊的权重并且为剩下的权重分配较低的精度水平</li>
</ul>
</li>
<li><code>SpQR</code>：引入了一个方法论：识别权重离群值，并且分配高精度，剩下的被量化到3bit</li>
<li><code>SqueezeLLM</code>： 在全精度稀疏矩阵中存储离群值，并且采用非均匀的量化方式量化剩下的权重</li>
<li><code>QuIP</code>：引入LDLQ
<ul>
<li>一个最优自适应方法解决二次代理目标</li>
<li>研究表明保证权重和海森矩阵之间的不相关性能提高LDLQ的效率
<ul>
<li>采用随机正交矩阵乘法来实现不相关性</li>
</ul>
</li>
</ul>
</li>
<li><code>FineQuant</code>：使用启发式的方法来决定每列的量化粒度，结合实验中获得的经验来设计量化方案</li>
<li><code>QuantEase</code>：在GPTQ上构建
<ul>
<li>当量化每一层使用坐标下降更精确地补偿未量化的权重</li>
<li>可以使用GPTQ做初始化以及进一步优化补偿过程</li>
</ul>
</li>
<li><code>LLM-MQ</code>：用FP16的格式保护权重离群值，并且把他们存储在压缩稀疏列格式（CSR）
<ul>
<li>将每层的位宽分配建模为整数规划问题，并采用高效求解器在几秒内完成求解</li>
<li>设计高效CUDA核心来集成反量化算子</li>
</ul>
</li>
</ul>
</li>
<li>Weight-activation 量化
<ul>
<li><code>Zeroquant</code>：采用核融合来最小化访存消耗，并进行逐层知识蒸馏来恢复其性能</li>
<li><code>FlexGen</code>：直接量化权重核KV cache 到INT4 来减少大批量推理过程中的内存占用</li>
<li><code>LLM.Int8</code>： LLM.int8() [194]发现激活值中的异常值集中在一小部分通道内。利用这一见解，LLM.int8() 根据输入通道内的离群值分布将激活和权重分为两个不同的部分，以最小化激活中的量化误差
<ul>
<li>带有离群值的部分使用FP16的格式进行存储，其他的则以INT8进行存储</li>
</ul>
</li>
<li><code>SmoothQuant</code> 采用重参数化的方法解决量化激活值的挑战
<ul>
<li>该方法引入了一个缩放因子，它在扩大权重通道数据范围的同时，缩小了相应激活通道的数据范围。</li>
</ul>
</li>
<li><code>ZeroQuant</code>：权重每组量化以及激活token量化。
<ul>
<li>ZeroQuantV2： 采用LoRC（Low-Rank Compensation），使用低秩矩阵来减少量化的不准确性</li>
</ul>
</li>
<li><code>RPTQ</code>：发现不同激活通道的分布存在显著差异，这给量化带来了挑战。为缓解此问题，RPTQ将具有相似激活分布的通道重组为簇，并在每个簇内独立应用量化。</li>
<li><code>OliVe</code>：观察到离群值附近的正常值不太重要。因此，它将每个离群值与一个正常值配对，牺牲后者以实现离群值更广泛的表示范围。</li>
<li><code>OS+</code> ： 每通道偏移和放缩</li>
<li><code>ZeroQuant-FP</code>:研究表明，将激活值量化为浮点类型（FP4和FP8）比整数类型产生更好的结果</li>
<li><code>Omniquant</code>:它优化权重裁剪的边界和等效变换的缩放因子，以最小化量化误差。</li>
<li><code>QLLM</code>: 通过实施通道重组解决了异常值对量化的影响，引入了可学习的低秩参数来最小化量化误差</li>
<li><code>Atom</code>：涉及激活的混合精度以及动态量化
<ul>
<li>拓展了KV cache 的量化方法到INT4</li>
</ul>
</li>
<li><code>LLM-FP4</code>： 将整个模型量化到FP4的格式并且引入预偏移指数偏置技术。该方法将激活值的缩放因子与权重相结合，以解决离群值带来的量化挑战。</li>
<li><code>BiLLM</code>： BiLLM[203]是迄今为止最低比特PTQ研究之一。BiLLM发现权重呈钟形分布，且权重的海森矩阵具有异常长的尾部分布。基于此，它提出根据海森矩阵将权重从结构上分为显著值和非显著值，并对它们分别进行二值化。因此，BiLLM可以将大型语言模型广泛量化至1.08比特，且不会导致困惑度显著下降。</li>
<li><code>KVQuant</code>：提出了一个非均匀的量化方法for KV cache。
<ul>
<li>通过在校正集上离线获得最优的数据形式</li>
</ul>
</li>
<li><code>KIVI</code>： 提出了一种不需要调整的2bit KV cache 量化算法
<ul>
<li>对key cache进行每个通道的量化，以及对于value cache进行每个token的量化，通过group-wise的方式</li>
</ul>
</li>
<li>Li et al.</li>
</ul>
</li>
</ul>
<h5 id="23212-量化感知训练"><a class="markdownIt-Anchor" href="#23212-量化感知训练"></a> 2.3.2.1.2. 量化感知训练</h5>
<p>将量化的影响融入模型训练的过程中，通过集成模拟量化效果的层，这种方法有助于权重适应量化引起的误差，从而提升任务性能</p>
<ul>
<li>
<p>为了减少数据的需求</p>
<ul>
<li><code>LLM-QAT</code>：
<ul>
<li>引入了一种不需要数据的方法：通过使用原始的FP16  LLMs来生成训练数据</li>
<li>具体而言，LLM-QAT将分词词汇表中的每个标记作为起始标记来生成句子。</li>
<li>基于生成的训练数据，LLM-QAT采用基于蒸馏的工作流程来训练量化的 LLM，使其匹配原始 FP16 LLM 的输出分布</li>
</ul>
</li>
<li><code>Norm Tweaking</code>：
<ul>
<li>将起始标记的选择限制为仅在比例最高的顶级语言类别中列出的那些语言类别。这种策略可以有效提高量化模型在不同任务上的泛化能力</li>
</ul>
</li>
</ul>
</li>
<li>
<p>为了减少计算成本，很多方法采用参数效率调整策略（PEFT）来加速QAT</p>
<ul>
<li><code>QLoRA</code>：将权重量化到4bit，然后针对每个4位权重矩阵采用BF16精度的LoRA来微调量化模型
<ul>
<li>允许在只是用30GB的GPU存储来完成65B的参数高效微调</li>
</ul>
</li>
<li><code>QALoRA</code>： 将分组量化和QLoRA结合
<ul>
<li>由于QLoRA的量化参数量小于LoRA的参数，导致量化与低秩自适应的不平衡，于是采用分组来解决这个问题</li>
<li>可以把LoRA项融入量化权重矩阵</li>
</ul>
</li>
<li><code>LoftQ</code>：使用原始FP16权重矩阵和量化后权重矩阵的差异的奇异值分解，初始化LoRA矩阵
<ul>
<li>实现更精确对于原始权重的估计</li>
</ul>
</li>
<li><code>Norm Tweaking</code>： 提出在量化后训练LayerNorm层，并使用知识蒸馏使量化模型的输出分布与FP16模型匹配，实现类似于LLM-QAT的效果，同时避免高昂的训练成本。</li>
</ul>
</li>
</ul>
<h5 id="23213-对比试验和分析"><a class="markdownIt-Anchor" href="#23213-对比试验和分析"></a> 2.3.2.1.3. 对比试验和分析</h5>
<blockquote>
<p>试验设置：采用了 weight-only 的量化方法（AWQ），使用LLaMA-2-7B 和 LLaMA-2-13B以及TensorRT-LLM以及LMDeploy推理框架</p>
</blockquote>
<ol>
<li>Weight-only 量化能大大加速解码阶段，这种增强主要源于更快速地从HBM加载低精度的权重</li>
<li>对于预填充阶段，weight-only量化可能会增加延迟，这是因为与填充阶段的瓶颈是计算消耗而不是存储消耗</li>
<li>随着batch size和输入长度的增长，weight-only量化的加速程度将逐渐消失
<ul>
<li>因为batch size和输入长度变大的时候，计算开销会增加，此时计算开销将会占主导，而访存开销将会降低</li>
</ul>
</li>
<li>Weight-only 量化由于能显著降低访存开销而对于LLM大有脾益</li>
</ol>
<h4 id="2322-稀疏化"><a class="markdownIt-Anchor" href="#2322-稀疏化"></a> 2.3.2.2. 稀疏化</h4>
<p>这个方法只在通过高效忽略零元素，降低计算的复杂度和内存使用<br>
通常被使用在权重参数和注意力激活</p>
<h5 id="23221-权重剪枝"><a class="markdownIt-Anchor" href="#23221-权重剪枝"></a> 2.3.2.2.1. 权重剪枝</h5>
<p><img src="/2025/10/01/llm/Pasted image 20251003124048.png" alt="img" width="500px"><br>
去除不那么重要的结构和权重，在不损失太多模型性能地在预填充和解码阶段降低计算和存储开销、</p>
<ul>
<li>Unstructred pruning
<ul>
<li>以细粒度剪枝单个权重值，对比结构化剪枝，他能实现更好的稀疏化，然而获得的稀疏模式缺乏高阶的规律性</li>
<li>可能会导致不规则的访问内存以及计算模式，可能会导致硬件加速受阻
<ul>
<li>重构误差
<ul>
<li><code>SparseGPT</code>： 它集成OBS的思想：考虑移除每一个权重的网络重构误差
<ul>
<li>OBS迭代地决定剪枝mask剪枝，并且重构未剪枝的权重来补偿剪枝的损失</li>
<li>通过Optimal Partial Update 技术来改进OBS</li>
</ul>
</li>
<li><code>Prune and Tune</code>：在SparseGPT上微调LLM</li>
<li><code>ISC</code>：结合OBS和OBD的标准</li>
<li><code>BESA</code>：通过重构误差的梯度下降来学习一个可微的二元掩码
<ul>
<li>剪枝率有最小化重构误差决定</li>
</ul>
</li>
</ul>
</li>
<li>基于幅度
<ul>
<li><code>Wanda</code>：提出将权重大小与输入激活范数之间的逐元素乘积用作剪枝准则</li>
<li><code>RIA</code>：通过使用相对重要性和激活度指标共同考虑权重和激活值，该指标基于每个权重元素的所有连接权重来评估其重要性。</li>
<li><code>OWL</code>：专注于决定每一层的剪枝率</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Structured pruning
<ul>
<li>剪枝一整个通道或者层，操作的粒度更粗</li>
<li>这些方法在传统硬件平台上能促进推理的加速</li>
<li>然而粗粒度通常会较大影响模型的性能</li>
<li><code>LLM-pruner</code>：提出一种任务无关的剪枝方法
<ul>
<li>具体来说，它首先基于神经元之间的连接依赖关系识别大型语言模型中的耦合结构。</li>
<li>然后，它基于精心设计的组级剪枝指标决定移除哪些结构组。</li>
<li>剪枝后，它进一步提出通过一种参数高效的训练技术，即LoRA等，来恢复模型性能</li>
</ul>
</li>
<li><code>Shared LLaMA</code>：提出将原始大语言模型剪枝为现有预训练大语言模型的特定目标架构
<ul>
<li>此外，它提出动态批加载技术</li>
</ul>
</li>
<li><code>ZipLM</code>：</li>
<li><code>LoRAPrune</code>：剪枝带有LoRA模块的预训练LLM。</li>
<li><code>LoRAShear</code>：</li>
<li><code>SliceGPT</code>：基于RMSNorm运算的计算不变性理念</li>
<li><code>PLATON</code>：提出剪枝时同时考虑他们的重要性和不确定性，它使用重要性分数的指数移动平均（EMA）来估计重要性，并采用上置信界（UCB）来表示不确定性</li>
<li><code>SIMPLE</code>：通过学习对应的稀疏掩码来进行注意力头，FFN神经元和隐藏维度的剪枝
<ul>
<li>剪枝后使用知识蒸馏来进行模型性能恢复</li>
</ul>
</li>
</ul>
</li>
<li>Sparse Attention：<br>
<img src="/2025/10/01/llm/Pasted image 20251003133231.png" alt="img" width="417px">
<ul>
<li><strong>Static sparse attention</strong>：独立于输入移除激活值。这些方法预先定义稀疏注意力掩码。
<ul>
<li>最常见的是局部注意力以及全局注意力模式
<ul>
<li>局部注意力模式：使用固定大小的窗口注意力捕捉局部上下文</li>
<li>全局注意力模式：模式通过计算并关注序列中的所有标记，捕获特定标记与所有其他标记的相关性。</li>
</ul>
</li>
<li><code>Sparse Transformer</code>：结合这些模式，用局部模式捕捉局部上下文，然后每隔几个词用全局模式聚合信息</li>
<li><code>StreamingLLM</code>：</li>
<li><code>BigBird</code>：使用随机模式，局部模式、全局模式和随机模式的组合被证明能够封装所有连续的序列到序列函数，这证实了其图灵完备性。</li>
<li><code>Longformer</code>：膨胀滑动窗口模式，它类似于膨胀卷积神经网络，使滑动窗口“膨胀”以增加感受野。</li>
<li><code>Structured Sparse Attention</code> ：提出了一种熵感知训练方法，该方法将高概率注意力值聚集到更密集的区域。</li>
<li><code>SemSA</code>：采用基于梯度的剖析方法识别重要的注意力模式，并自动优化注意力密度分布，以进一步提升模型效率。</li>
</ul>
</li>
<li><strong>Dynamic sparse attention</strong>：对不同的输入会自适应地去除激活值，通过实时监测神经元激活值来绕过影响可忽略的神经元的计算，从而实现剪枝。
<ul>
<li><code>Token-pruning</code>：Spatten等[153]、SeqBoat等[154]和Adaptively Sparse Attention等[155]利用语言结构中固有的冗余性，提出了动态令牌级剪枝策略。
<ul>
<li><code>Spatten</code>等[153]通过聚合注意力矩阵列来评估每个词的累积重要性，随后在后续层中从输入中剪枝累积重要性最小的令牌。</li>
<li><code>SeqBoat</code>等[154]使用稀疏sigmoid函数训练线性状态空间模型（SSM），以确定每个注意力头要剪枝的令牌。Spatten和SeqBoat都会对整个输入中无信息的令牌进行剪枝。</li>
<li><code>Adaptively Sparse Attention</code>等[155]在生成过程中逐步剪枝令牌，它会丢弃未来生成不再需要的部分上下文。</li>
</ul>
</li>
<li>Attention-pruning：
<ul>
<li>如图11(d)所示，这些方法并非剪枝特定标记的所有注意力值，而是基于输入动态剪枝注意力的选择性部分。</li>
<li>该领域内一种重要的方法是将输入标记动态分割成组（称为桶），并策略性地省略位于不同桶中的标记的注意力计算。这些方法的挑战和重点在于如何将相关标记聚类在一起，从而仅在它们之间进行注意力计算以提高效率。</li>
<li><code>Reformer</code>[156]利用局部敏感哈希将具有相同哈希码的键和查询聚类到同一个桶中。</li>
<li>在此基础上，<code>Sparse Flash Attention</code>[157]引入了专门针对这种基于哈希的稀疏注意力机制优化的GPU内核，进一步提高了计算效率。</li>
<li>同时，<code>Routing Transformer</code>[158]采用球形k均值聚类算法将标记聚合到桶中，优化了注意力计算的选择过程。</li>
<li><code>Sparse Sinkhorn Attention</code>[159]采用学习排序网络将键与相关的查询桶对齐，确保仅在相应的查询-键对之间计算注意力。</li>
<li>与桶级操作不同，<code>H2O</code>[160]引入了标记级动态注意力剪枝机制。它将静态局部注意力与当前查询和一组动态识别的关键标记（称为重击者（H2））之间的动态计算相结合。这些重击者通过逐出策略动态调整，旨在在每个生成步骤中移除最不重要的键，有效管理重击者集合的大小和相关性。</li>
</ul>
</li>
<li>此外，将每个标记视为图节点并将标记之间的注意力视为边，为静态稀疏注意力提供了扩展视角[150]，[161]。原始的全注意力机制相当于具有均匀最短路径距离1的完全图。稀疏注意力通过其随机掩码引入随机边，有效地将任意两个节点之间的最短路径距离减少到O(logn)，从而保持类似于全注意力的高效信息流。Diffuser[161]利用图论视角，通过多跳标记相关性扩展稀疏注意力的感受野。它还从扩展器图特性中汲取灵感，设计出更优的稀疏模式，以近似全注意力的信息流。</li>
<li><code>Spatten</code>[153]还将剪枝从令牌粒度扩展到注意力头粒度，消除非必要注意力头的计算，以进一步降低计算和内存需求</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="2323-结构优化"><a class="markdownIt-Anchor" href="#2323-结构优化"></a> 2.3.2.3 结构优化</h4>
<h5 id="23231-神经架构搜索-nas"><a class="markdownIt-Anchor" href="#23231-神经架构搜索-nas"></a> 2.3.2.3.1. 神经架构搜索 (NAS)</h5>
<p>旨在自动搜索最优的神经架构来平衡性能和效率</p>
<ul>
<li><code>AutoTinyBERT</code> [135]利用一次性神经架构搜索（NAS）来发现Transformer架构的超参数。值得注意的是，它引入了一种引人注目的批次训练方法来训练超级预训练语言模型（SuperPLM），随后采用进化算法来识别最优子模型。</li>
<li><code>NAS-BERT</code> [136] 使用多种创新技术，如块级搜索、搜索空间剪枝和性能近似，在传统的自监督预训练任务上训练大型超级网络。这种方法使NAS-BERT能够高效应用于各种下游任务，而无需大量重新训练。通过NAS进行结构剪枝 [137] 将结构剪枝视为多目标NAS问题，并通过一次性NAS方法解决。</li>
<li><code>LiteTransformerSearch</code> [138]提出使用无需训练的指标，即参数数量，作为代理指标来指导搜索。该方法能够高效探索和选择最优架构，且在搜索阶段无需实际训练。</li>
<li><code>AutoDistil</code> [139]提出了一种完全与任务无关的少样本NAS算法，其特点是三种主要技术：搜索空间划分、与任务无关的SuperLM训练和与任务无关的搜索。这种方法旨在通过最少的任务特定适应，促进跨各种任务的高效架构发现。通常，NAS算法需要评估每个采样架构的性能，这可能会产生显著的训练成本。因此，这些技术难以应用于大型语言模型。</li>
</ul>
<h5 id="23232-低秩分解-lrf"><a class="markdownIt-Anchor" href="#23232-低秩分解-lrf"></a> 2.3.2.3.2. 低秩分解 (LRF)</h5>
<p><img src="/2025/10/01/llm/Pasted image 20251003140400.png" alt="img" width="659px"></p>
<ul>
<li><code>LoRD</code>表明，通过低秩分解（LRF）压缩大型语言模型（LLMs）具有潜力，且不会大幅降低性能。具体而言，它采用奇异值分解（SVD）对权重矩阵进行因式分解，并成功将一个拥有160亿参数的大型语言模型压缩至123亿，性能下降极小。</li>
<li><code>TensorGPT</code>提出了一种使用张量列车分解来压缩嵌入层的方法。每个标记嵌入都被视为矩阵乘积态（MPS），并以分布式方式高效计算。</li>
<li><code>LoSparse</code>结合了低秩分解和权重剪枝在大型语言模型压缩方面的优势。通过利用低秩近似，LoSparse降低了直接模型剪枝中通常会出现的丢失过多表达性神经元的风险。</li>
<li>LPLR 和 ZeroQuant-V2均提出通过同时对权重矩阵应用低秩分解和量化来对其进行压缩。</li>
<li><code>DSFormer</code>提出将权重矩阵分解为半结构化稀疏矩阵和小型密集矩阵的乘积。</li>
<li><code>ASVD</code>设计了一种激活感知的 SVD方法。该方法包括在应用 SVD进行矩阵分解之前，基于激活分布对权重矩阵进行缩放。ASVD还包括通过搜索过程为每个层确定合适的截断秩</li>
</ul>
<h4 id="2324-知识蒸馏"><a class="markdownIt-Anchor" href="#2324-知识蒸馏"></a> 2.3.2.4. 知识蒸馏</h4>
<p><img src="/2025/10/01/llm/Pasted image 20251003140905.png" alt="img" width="670px"></p>
<h5 id="23241-白盒知识蒸馏"><a class="markdownIt-Anchor" href="#23241-白盒知识蒸馏"></a> 2.3.2.4.1. 白盒知识蒸馏</h5>
<p>利用教师模型结构和参数访问权限的蒸馏方法。该方法使知识蒸馏能够有效利用教师模型的中间特征和输出logits，从而提升学生模型的性能。</p>
<ul>
<li><code>MiniLLM</code> [129]提出采用标准的白盒知识蒸馏方法，但将正向KL散度（KLD）替换为反向KL散度。</li>
<li><code>GKD</code>[130]引入了策略内数据的使用，其中包括学生模型自身生成的输出序列，以进一步蒸馏学生模型。该方法侧重于使用这些策略内数据对齐教师和学生模型之间的输出logits。</li>
<li><code>TED</code> [131]提出了一种任务感知的分层知识蒸馏方法。该方法包括在教师和学生模型的每一层之后添加过滤器，训练这些特定于任务的过滤器，随后冻结教师模型的过滤器，同时训练学生过滤器以使其输出特征与相应的教师过滤器对齐。</li>
<li><code>MiniMoE</code> [133] 通过将混合专家（Mixture-of-Experts，MoE）模型用作学生模型来缓解容量差距。</li>
<li>对于新出现的实体，预训练语言模型（LLMs）可能缺乏最新信息。为了解决这个问题，一种解决方案是在提示中加入额外的检索文本，尽管这会增加推理成本。或者，KPTD [134]建议通过知识蒸馏将实体定义中的知识转移到LLM参数中。该方法基于实体定义生成转移集，并蒸馏学生模型，使其基于这些定义与教师模型的输出分布相匹配。</li>
</ul>
<h5 id="23242-黑盒知识蒸馏"><a class="markdownIt-Anchor" href="#23242-黑盒知识蒸馏"></a> 2.3.2.4.2. 黑盒知识蒸馏</h5>
<p>黑箱知识蒸馏是指教师模型的结构和参数不可用的知识蒸馏方法.通常，黑盒知识蒸馏仅使用教师模型获得的最终结果。</p>
<ul>
<li>关于ICL能力，
<ul>
<li><code>Multitask-ICT</code>[116]引入了上下文学习蒸馏，以迁移大型语言模型（LLMs）的多任务少样本能力，同时利用了上下文学习和语言建模的优势。</li>
<li><code>MCKD[</code>117]观察到，从经过上下文学习的教师模型中蒸馏出的学生模型在未见过的输入提示上通常表现出更优的性能。基于这一观察，MCKD设计了一种多阶段蒸馏范式，其中前一阶段的学生模型被用于为后续阶段生成蒸馏数据，从而提高蒸馏方法的有效性。</li>
</ul>
</li>
<li>为了提炼思维链（CoT）推理能力，一些技术如Distilling Step-by-Step [118]、SCoTD [119]、CoT Prompting [120]、MCC-KD [121]和Fine-tune-CoT [122]提出了蒸馏方法，这些方法结合了从大型语言模型中提取的响应和推理过程来训练学生模型。
<ul>
<li><code>Socratic CoT</code> [123]也致力于将推理能力迁移到更小的模型中。具体而言，它对一对学生模型进行微调，即问题生成（QG）模型和问答（QA）模型。QG模型经过训练，能够基于输入问题生成中间问题，从而引导QA模型生成最终响应。</li>
<li><code>PaD</code> [124]观察到错误推理（即最终答案正确但推理步骤错误）可能对学生模型有害。为了解决这个问题，PaD提出为推理问题生成合成程序，然后可以通过额外的解释器对这些程序进行自动检查。这种方法有助于去除具有错误推理的蒸馏数据，提高学生模型训练数据的质量。</li>
</ul>
</li>
<li>针对指令跟随能力，已有多种方法被提出以将这种能力迁移到更小的模型中。
<ul>
<li><code>DISCO</code> [126]引入了一种技术，即使用大型语言模型生成短语扰动。然后，由特定任务的教师模型对这些扰动进行筛选，以提炼高质量的反事实数据。</li>
<li><code>LaMini-LM</code> [127]旨在通过设计多样化的指令集来迁移指令跟随能力，从而对学生模型进行蒸馏。Lion [128]利用教师模型识别困难指令，并生成新的复杂指令来对小型模型进行蒸馏。</li>
</ul>
</li>
</ul>
<h4 id="2325-动态推理"><a class="markdownIt-Anchor" href="#2325-动态推理"></a> 2.3.2.5. 动态推理</h4>
<p>本节重点介绍早期退出技术，该技术使大型语言模型能够根据特定样本或标记在不同的模型层停止推理。值得注意的是，虽然混合专家技术（在5.1.1节讨论）在推理过程中也会调整模型结构，但它们通常需要高昂的预训练成本。相比之下，早期退出技术只需训练一个小型模块来决定何时结束推理。我们将早期退出技术的研究分为两种主要类型：样本级早期退出和标记级早期退出（如图13所示）。<br>
<img src="/2025/10/01/llm/Pasted image 20251003142229.png" alt="img" width="456px"></p>
<h5 id="23251-采样级"><a class="markdownIt-Anchor" href="#23251-采样级"></a> 2.3.2.5.1. 采样级</h5>
<p>对于每一个输入采样,决定最优尺寸和大语言模型结构</p>
<ul>
<li>一种常见的方法是在每一层之后为大型语言模型增加额外的模块，利用这些模块来决定是否在特定层终止推理。
<ul>
<li><code>FastBERT</code>、<code>DeeBERT</code>、<code>MP</code>和<code>MPEE</code>直接训练这些模块，使其基于当前层的特征做出决策（例如，输出0继续或1停止）。</li>
<li><code>Global Past-Future Early Exit</code>提出了一种方法，通过来自前序层和后续层的语言信息丰富这些模块的输入。由于推理过程中无法直接获取后续层特征，因此训练了一个简单的前馈层来估计这些未来特征。</li>
<li><code>PABEE</code> 将模块训练为直接预测的输出头，当预测结果保持一致时建议终止推理。</li>
<li><code>HASHEE</code>采用了一种非参数决策方法，其基于的假设是相似样本应在同一层退出推理。</li>
</ul>
</li>
</ul>
<h5 id="23252-token级"><a class="markdownIt-Anchor" href="#23252-token级"></a> 2.3.2.5.2. Token级</h5>
<p>在大语言模型推理的解码阶段， tokens是按顺序生成的，token级早退技术旨在为每个输出 token优化大语言模型的大小和结构。</p>
<ul>
<li><code>CALM</code> [108] 在每个 Transformer层后引入早退分类器，训练它们输出置信度分数，以决定是否在特定层停止推理。值得注意的是，在自注意力块中，计算每一层当前 token 的特征依赖于同一层中所有先前 token 的特征（即 KV缓存）。为解决因先前 token早退而导致的 KV缓存缺失问题，CALM提出将特征从退出层直接复制到后续层，实验结果表明性能仅略有下降。</li>
<li><code>SkipDecode</code> [109]解决了先前早退方法的局限性，这些局限性阻碍了它们在批量推理和 KV缓存中的适用性，从而限制了实际的加速增益。对于批量推理，SkipDecode为批次内的所有 token提出了统一的退出点。关于 KV缓存，SkipDecode确保退出点单调递减，以防止 KV缓存的重新计算，从而提高推理效率</li>
</ul>
<h4 id="233-建议和未来展望"><a class="markdownIt-Anchor" href="#233-建议和未来展望"></a> 2.3.3. 建议和未来展望</h4>
<ul>
<li>在高效结构设计领域，寻找Transformer的替代架构是一个新兴的研究方向。诸如Mamba [73]、RWKV [60]及其各自变体[101]、[104]等模型已在各类任务中展现出具有竞争力的性能，近年来备受关注。尽管如此，研究这些非Transformer模型是否可能比Transformer模型存在某些缺点仍然具有现实意义。同时，探索非Transformer架构与注意力操作的融合[74]、[103]、[216]是未来研究的另一个有前景的方向。</li>
<li>在模型压缩领域，量化是大语言模型（LLM）部署中主要采用的方法，这主要归因于两个关键因素。首先，量化为压缩LLM提供了一种便捷的方式。例如，采用训练后量化（PTQ）方法可在几分钟内将拥有70亿参数的LLM压缩为更小的形式。其次，量化有望大幅降低内存消耗并提高推理速度，同时仅带来微小的性能损失。这种权衡对于许多实际应用而言通常是可接受的。然而，值得注意的是，量化仍可能损害LLM的某些涌现能力，如自校准或多步推理能力。此外，在处理长上下文等特定场景中，量化可能导致显著的性能下降[204]。因此，需要谨慎选择合适的量化方法，以减轻这些特定情况下的性能下降风险</li>
<li>大量文献致力于研究稀疏注意力技术，以实现高效的长上下文处理。例如，最近的代表性工作StreamingLLM [148]仅通过恢复几个注意力汇聚标记就能处理400万个标记。尽管如此，这些方法往往会牺牲关键信息，导致性能下降。因此，在高效管理长上下文的同时保留必要信息，仍是未来探索的重要领域。关于权重剪枝技术，LLM-KICK [217]指出，当前最先进的方法即使在相对较低的稀疏率下也会经历显著的性能下降。因此，开发有效的权重剪枝方法以维持LLM性能，仍是一个新兴且关键的研究方向</li>
<li>模型结构的优化通常涉及神经架构搜索（NAS）的使用，这通常需要大量的计算资源，对其在大型语言模型（LLMs）压缩中的实际应用构成了潜在障碍。因此，研究采用自动结构优化进行大型语言模型压缩的可行性值得进一步探索。此外，低秩分解（LRF）等技术在压缩率和任务性能之间实现最佳平衡仍然面临挑战。例如，ASVD [146]在不损害大型语言模型推理能力的情况下，仅能实现10%至20%的适度压缩率。</li>
<li>除了采用单独的模型压缩技术外，多项研究还探索了不同方法的组合以压缩大型语言模型，利用它们各自的优势来提高效率。例如，MPOE [88]专门对基于混合专家（MoE）的大型语言模型中的专家前馈网络（FFNs）应用权重矩阵分解，旨在进一步降低内存需求。LLM-MQ [191]利用权重稀疏性技术在模型量化过程中保护权重离群值，从而最大限度地减少量化误差。LPLR [143]专注于对低秩分解的权重矩阵进行量化，以进一步降低大型语言模型推理过程中的内存占用和内存访问成本。此外，LoSparse [142] 将低秩分解与权重剪枝相结合，利用剪枝来增强低秩近似的多样性，同时通过低秩分解保留重要权重并防止关键信息丢失。这些方法凸显了集成多种压缩技术以实现大型语言模型更好优化的潜力。</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://Ken1301225.github.io">Ken</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://ken1301225.github.io/2025/10/01/llm/">https://ken1301225.github.io/2025/10/01/llm/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://Ken1301225.github.io" target="_blank">KEN</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/llm-infra/">llm infra</a></div><div class="post-share"><div class="social-share" data-image="/2025/10/01/llm/Pasted%20image%2020251003140905.png" data-sites="wechat,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/10/01/llm_basic/" title="llm 基础知识"><img class="cover" src="/2025/10/01/llm_basic/Pasted%20image%2020251003144355.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">llm 基础知识</div></div><div class="info-2"><div class="info-item-1">关于大模型的基础知识调研和学习,其中包括多种注意力机制的了解,以及MoE架构,LoRA微调</div></div></div></a><a class="pagination-related" href="/2025/04/01/game/" title="构想的游戏-k小镇"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">构想的游戏-k小镇</div></div><div class="info-2"><div class="info-item-1">ai控制一整个小镇?🤔</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2-llm%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96"><span class="toc-text"> 2. LLM推理优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#21-%E5%88%86%E7%B1%BB"><span class="toc-text"> 2.1. 分类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-%E7%B3%BB%E7%BB%9F%E5%B1%82%E9%9D%A2%E7%9A%84%E4%BC%98%E5%8C%96"><span class="toc-text"> 2.2 系统层面的优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#221-%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E"><span class="toc-text"> 2.2.1. 推理引擎</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2211-%E5%9B%BE%E5%92%8C%E7%AE%97%E5%AD%90%E4%BC%98%E5%8C%96"><span class="toc-text"> 2.2.1.1 🧐图和算子优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2212-%E6%8E%A8%E6%B5%8B%E8%A7%A3%E7%A0%81%E6%8A%95%E6%9C%BA%E8%A7%A3%E7%A0%81"><span class="toc-text"> 2.2.1.2. 🧐推测解码(投机解码)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#222-%E6%9C%8D%E5%8A%A1%E7%B3%BB%E7%BB%9F"><span class="toc-text"> 2.2.2. 服务系统</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2221-%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86"><span class="toc-text"> 2.2.2.1 存储管理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2222-%E8%BF%9E%E7%BB%AD%E6%89%B9%E5%A4%84%E7%90%86"><span class="toc-text"> 2.2.2.2 连续批处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2223-%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5"><span class="toc-text"> 2.2.2.3 🧐调度策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2224-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F"><span class="toc-text"> 2.2.2.4. 分布式系统</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#223-llm-%E6%A1%86%E6%9E%B6%E5%AF%B9%E6%AF%94"><span class="toc-text"> 2.2.3. LLM 框架对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#224-%E6%9C%AA%E6%9D%A5%E7%9A%84%E6%96%B9%E5%90%91%E4%BB%A5%E5%8F%8A%E4%B8%80%E4%BA%9B%E5%BB%BA%E8%AE%AE"><span class="toc-text"> 2.2.4. 未来的方向以及一些建议</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#23-%E6%A8%A1%E5%9E%8B%E5%B1%82%E6%AC%A1%E7%9A%84%E4%BC%98%E5%8C%96"><span class="toc-text"> 2.3. 模型层次的优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#231-%E9%AB%98%E6%95%88%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="toc-text"> 2.3.1. 🧐高效结构设计</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2311-%E9%AB%98%E6%95%88ffn%E8%AE%BE%E8%AE%A1"><span class="toc-text"> 2.3.1.1. 高效FFN设计</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2312-%E9%AB%98%E6%95%88%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%AE%BE%E8%AE%A1"><span class="toc-text"> 2.3.1.2. 高效注意力设计.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2313-%E6%9B%BF%E4%BB%A3transformer"><span class="toc-text"> 2.3.1.3. 🧐替代Transformer</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#232-%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9"><span class="toc-text"> 2.3.2. 模型压缩</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2321-%E9%87%8F%E5%8C%96"><span class="toc-text"> 2.3.2.1 量化</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#23211-%E5%90%8E%E8%AE%AD%E7%BB%83%E9%87%8F%E5%8C%96"><span class="toc-text"> 2.3.2.1.1. 后训练量化</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#23212-%E9%87%8F%E5%8C%96%E6%84%9F%E7%9F%A5%E8%AE%AD%E7%BB%83"><span class="toc-text"> 2.3.2.1.2. 量化感知训练</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#23213-%E5%AF%B9%E6%AF%94%E8%AF%95%E9%AA%8C%E5%92%8C%E5%88%86%E6%9E%90"><span class="toc-text"> 2.3.2.1.3. 对比试验和分析</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2322-%E7%A8%80%E7%96%8F%E5%8C%96"><span class="toc-text"> 2.3.2.2. 稀疏化</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#23221-%E6%9D%83%E9%87%8D%E5%89%AA%E6%9E%9D"><span class="toc-text"> 2.3.2.2.1. 权重剪枝</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2323-%E7%BB%93%E6%9E%84%E4%BC%98%E5%8C%96"><span class="toc-text"> 2.3.2.3 结构优化</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#23231-%E7%A5%9E%E7%BB%8F%E6%9E%B6%E6%9E%84%E6%90%9C%E7%B4%A2-nas"><span class="toc-text"> 2.3.2.3.1. 神经架构搜索 (NAS)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#23232-%E4%BD%8E%E7%A7%A9%E5%88%86%E8%A7%A3-lrf"><span class="toc-text"> 2.3.2.3.2. 低秩分解 (LRF)</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2324-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F"><span class="toc-text"> 2.3.2.4. 知识蒸馏</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#23241-%E7%99%BD%E7%9B%92%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F"><span class="toc-text"> 2.3.2.4.1. 白盒知识蒸馏</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#23242-%E9%BB%91%E7%9B%92%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F"><span class="toc-text"> 2.3.2.4.2. 黑盒知识蒸馏</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2325-%E5%8A%A8%E6%80%81%E6%8E%A8%E7%90%86"><span class="toc-text"> 2.3.2.5. 动态推理</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#23251-%E9%87%87%E6%A0%B7%E7%BA%A7"><span class="toc-text"> 2.3.2.5.1. 采样级</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#23252-token%E7%BA%A7"><span class="toc-text"> 2.3.2.5.2. Token级</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#233-%E5%BB%BA%E8%AE%AE%E5%92%8C%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B"><span class="toc-text"> 2.3.3. 建议和未来展望</span></a></li></ol></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2019 - 2025 By Ken</span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'neutral' : 'neutral'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>